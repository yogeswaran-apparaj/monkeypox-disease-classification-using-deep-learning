{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13730831,"sourceType":"datasetVersion","datasetId":8736059}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport pickle\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.preprocessing import label_binarize\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport torchvision\nfrom torchvision import transforms, datasets\nimport torch.backends.cudnn as cudnn\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport timm\nimport optuna\nfrom torch.cuda import amp\n\nos.makedirs('/kaggle/working/artifacts', exist_ok=True)\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\nset_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nclass AlbumentationsDataset(Dataset):\n    def __init__(self, file_paths, labels, transform=None, class_to_idx=None):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n        self.class_to_idx = class_to_idx\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        image = Image.open(self.file_paths[idx]).convert('RGB')\n        image = np.array(image)\n        label = self.labels[idx]\n        \n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n            \n        return image, label\n\ndef get_transforms():\n    train_transform = A.Compose([\n        A.Resize(224, 224),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.3),\n        A.RandomBrightnessContrast(p=0.3),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    val_transform = A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    return train_transform, val_transform\n\ndef create_datasets():\n    data_dir = '/kaggle/input/multiclass-skin-disease-dataset/Skin Lesion Dataset'\n    train_dir = os.path.join(data_dir, 'train')\n    val_dir = os.path.join(data_dir, 'val')\n    test_dir = os.path.join(data_dir, 'test')\n    \n    train_dataset = datasets.ImageFolder(train_dir)\n    val_dataset = datasets.ImageFolder(val_dir)\n    test_dataset = datasets.ImageFolder(test_dir)\n    \n    class_names = train_dataset.classes\n    class_to_idx = train_dataset.class_to_idx\n    \n    train_paths = [img[0] for img in train_dataset.imgs]\n    train_labels = [img[1] for img in train_dataset.imgs]\n    \n    val_paths = [img[0] for img in val_dataset.imgs]\n    val_labels = [img[1] for img in val_dataset.imgs]\n    \n    test_paths = [img[0] for img in test_dataset.imgs]\n    test_labels = [img[1] for img in test_dataset.imgs]\n    \n    train_transform, val_transform = get_transforms()\n    \n    train_albumentations_dataset = AlbumentationsDataset(\n        train_paths, train_labels, train_transform, class_to_idx\n    )\n    val_albumentations_dataset = AlbumentationsDataset(\n        val_paths, val_labels, val_transform, class_to_idx\n    )\n    test_albumentations_dataset = AlbumentationsDataset(\n        test_paths, test_labels, val_transform, class_to_idx\n    )\n    \n    return (train_albumentations_dataset, val_albumentations_dataset, \n            test_albumentations_dataset, class_names)\n\nclass EarlyStopping:\n    def __init__(self, patience=7, verbose=False, delta=0, path='/kaggle/working/best_model.pth'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        \n    def __call__(self, val_loss, model):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n            \n    def save_checkpoint(self, val_loss, model):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss\n\nclass BaselineModel(nn.Module):\n    def __init__(self, num_classes=6):\n        super(BaselineModel, self).__init__()\n        self.backbone = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n        in_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Linear(in_features, num_classes)\n        \n    def forward(self, x):\n        return self.backbone(x)\n\nclass HybridViTModel(nn.Module):\n    def __init__(self, num_classes=6, transformer_depth=4, dropout=0.1):\n        super(HybridViTModel, self).__init__()\n        self.resnet = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)\n        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n        \n        self.feature_dim = 2048\n        self.patch_size = 7\n        self.num_patches = (224 // self.patch_size) ** 2\n        self.patch_embed_dim = 512\n        \n        self.patch_embed = nn.Conv2d(\n            self.feature_dim, self.patch_embed_dim, \n            kernel_size=self.patch_size, stride=self.patch_size\n        )\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.patch_embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.patch_embed_dim))\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.patch_embed_dim,\n            nhead=8,\n            dim_feedforward=1024,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_depth)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(self.patch_embed_dim, num_classes)\n        \n        self._init_weights()\n        \n    def _init_weights(self):\n        nn.init.normal_(self.cls_token, std=0.02)\n        nn.init.normal_(self.pos_embed, std=0.02)\n        nn.init.xavier_uniform_(self.patch_embed.weight)\n        if self.patch_embed.bias is not None:\n            nn.init.constant_(self.patch_embed.bias, 0)\n            \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        features = self.resnet(x)\n        \n        patch_embeds = self.patch_embed(features)\n        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n        \n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, patch_embeds), dim=1)\n        \n        # Ensure positional embeddings match the sequence length\n        if x.size(1) != self.pos_embed.size(1):\n            pos_embed = self.pos_embed[:, :x.size(1), :]\n        else:\n            pos_embed = self.pos_embed\n            \n        x = x + pos_embed\n        \n        x = self.dropout(x)\n        x = self.transformer(x)\n        \n        cls_output = x[:, 0]\n        output = self.classifier(cls_output)\n        \n        return output\n\ndef train_epoch(model, dataloader, criterion, optimizer, device, scaler):\n    model.train()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for batch_idx, (data, target) in enumerate(pbar):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        \n        with torch.amp.autocast('cuda'):\n            output = model(data)\n            loss = criterion(output, target)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        running_loss += loss.item()\n        _, predicted = output.max(1)\n        total_samples += target.size(0)\n        correct_predictions += predicted.eq(target).sum().item()\n        \n        pbar.set_postfix({\n            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n            'Acc': f'{100.*correct_predictions/total_samples:.2f}%'\n        })\n    \n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = 100. * correct_predictions / total_samples\n    \n    return epoch_loss, epoch_acc\n\ndef validate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for data, target in tqdm(dataloader, desc='Validation'):\n            data, target = data.to(device), target.to(device)\n            \n            with torch.amp.autocast('cuda'):\n                output = model(data)\n                loss = criterion(output, target)\n            \n            running_loss += loss.item()\n            _, predicted = output.max(1)\n            total_samples += target.size(0)\n            correct_predictions += predicted.eq(target).sum().item()\n    \n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = 100. * correct_predictions / total_samples\n    \n    return epoch_loss, epoch_acc\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, early_stopping, epochs, device):\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    scaler = torch.amp.GradScaler('cuda')\n    \n    for epoch in range(epochs):\n        print(f'Epoch {epoch+1}/{epochs}')\n        \n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, scaler)\n        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n        \n        if scheduler:\n            scheduler.step()\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_accs.append(train_acc)\n        val_accs.append(val_acc)\n        \n        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n        print('-' * 50)\n        \n        early_stopping(val_loss, model)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered\")\n            break\n    \n    return {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_accs': train_accs,\n        'val_accs': val_accs\n    }\n\ndef objective(trial):\n    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n    transformer_depth = trial.suggest_int('transformer_depth', 2, 6)\n    \n    train_dataset, val_dataset, _, class_names = create_datasets()\n    \n    batch_size = 32\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    model = HybridViTModel(\n        num_classes=len(class_names),\n        transformer_depth=transformer_depth,\n        dropout=dropout\n    ).to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = CosineAnnealingLR(optimizer, T_max=10)\n    \n    early_stopping = EarlyStopping(patience=5, verbose=False)\n    \n    try:\n        history = train_model(\n            model, train_loader, val_loader, criterion, optimizer, \n            scheduler, early_stopping, epochs=10, device=device\n        )\n        best_val_acc = max(history['val_accs'])\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            torch.cuda.empty_cache()\n            return float('inf')\n        else:\n            raise e\n    \n    return -best_val_acc\n\ndef evaluate_model(model, test_loader, device, class_names):\n    model.eval()\n    all_predictions = []\n    all_targets = []\n    all_probabilities = []\n    \n    with torch.no_grad():\n        for data, target in tqdm(test_loader, desc='Testing'):\n            data, target = data.to(device), target.to(device)\n            \n            output = model(data)\n            probabilities = torch.softmax(output, dim=1)\n            _, predicted = output.max(1)\n            \n            all_predictions.extend(predicted.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n            all_probabilities.extend(probabilities.cpu().numpy())\n    \n    all_predictions = np.array(all_predictions)\n    all_targets = np.array(all_targets)\n    all_probabilities = np.array(all_probabilities)\n    \n    accuracy = 100. * (all_predictions == all_targets).sum() / len(all_targets)\n    \n    print(f'Test Accuracy: {accuracy:.2f}%')\n    print('\\nClassification Report:')\n    print(classification_report(all_targets, all_predictions, target_names=class_names))\n    \n    cm = confusion_matrix(all_targets, all_predictions)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/artifacts/confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    y_test_bin = label_binarize(all_targets, classes=range(len(class_names)))\n    \n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    plt.figure(figsize=(10, 8))\n    for i in range(len(class_names)):\n        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], all_probabilities[:, i])\n        roc_auc[i] = roc_auc_score(y_test_bin[:, i], all_probabilities[:, i])\n        plt.plot(fpr[i], tpr[i], lw=2, \n                label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Multi-class ROC Curves')\n    plt.legend(loc=\"lower right\")\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/artifacts/roc_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    metrics = {\n        'accuracy': accuracy,\n        'per_class_auc': {class_names[i]: float(roc_auc[i]) for i in range(len(class_names))},\n        'macro_auc': float(roc_auc_score(y_test_bin, all_probabilities, average='macro')),\n        'weighted_auc': float(roc_auc_score(y_test_bin, all_probabilities, average='weighted'))\n    }\n    \n    return metrics, all_predictions, all_targets, all_probabilities\n\ndef create_streamlit_app(class_names):\n    app_code = '''\nimport streamlit as st\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport matplotlib.pyplot as plt\nimport json\n\nclass HybridViTModel(nn.Module):\n    def __init__(self, num_classes=6, transformer_depth=4, dropout=0.1):\n        super(HybridViTModel, self).__init__()\n        from torchvision import models\n        self.resnet = models.resnet50(weights=None)\n        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n        \n        self.feature_dim = 2048\n        self.patch_size = 7\n        self.num_patches = (224 // self.patch_size) ** 2\n        self.patch_embed_dim = 512\n        \n        self.patch_embed = nn.Conv2d(\n            self.feature_dim, self.patch_embed_dim, \n            kernel_size=self.patch_size, stride=self.patch_size\n        )\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.patch_embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.patch_embed_dim))\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.patch_embed_dim,\n            nhead=8,\n            dim_feedforward=1024,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_depth)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(self.patch_embed_dim, num_classes)\n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        features = self.resnet(x)\n        patch_embeds = self.patch_embed(features)\n        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, patch_embeds), dim=1)\n        if x.size(1) != self.pos_embed.size(1):\n            pos_embed = self.pos_embed[:, :x.size(1), :]\n        else:\n            pos_embed = self.pos_embed\n        x = x + pos_embed\n        x = self.dropout(x)\n        x = self.transformer(x)\n        cls_output = x[:, 0]\n        output = self.classifier(cls_output)\n        return output\n\n@st.cache_resource\ndef load_model():\n    model = HybridViTModel(num_classes=6)\n    model.load_state_dict(torch.load('/kaggle/working/best_model.pth', map_location='cpu'))\n    model.eval()\n    return model\n\ndef preprocess_image(image):\n    transform = A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    image = np.array(image.convert('RGB'))\n    transformed = transform(image=image)\n    return transformed['image'].unsqueeze(0)\n\nst.set_page_config(page_title=\"Skin Disease Classifier\", layout=\"wide\")\nst.title(\"ðŸ§¬ Skin Disease Classification App\")\nst.write(\"Upload an image of a skin lesion to classify it into one of 6 categories.\")\n\nuploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n\nif uploaded_file is not None:\n    image = Image.open(uploaded_file)\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n    \n    with col2:\n        with st.spinner('Classifying...'):\n            model = load_model()\n            processed_image = preprocess_image(image)\n            \n            with torch.no_grad():\n                outputs = model(processed_image)\n                probabilities = torch.softmax(outputs, dim=1)\n            \n            probs = probabilities[0].numpy()\n            class_names = [\"Chickenpox\", \"Cowpox\", \"HFMD\", \"Healthy\", \"Measles\", \"Monkeypox\"]\n            \n            top3_indices = np.argsort(probs)[-3:][::-1]\n            top3_classes = [class_names[i] for i in top3_indices]\n            top3_probs = [probs[i] for i in top3_indices]\n            \n            st.subheader(\"ðŸ” Prediction Results\")\n            st.success(f\"**Predicted Class:** {top3_classes[0]} ({top3_probs[0]*100:.2f}%)\")\n            \n            st.subheader(\"ðŸ“Š Top 3 Predictions\")\n            for i, (cls, prob) in enumerate(zip(top3_classes, top3_probs)):\n                st.write(f\"{i+1}. {cls}: {prob*100:.2f}%\")\n            \n            st.subheader(\"ðŸ“ˆ All Class Probabilities\")\n            fig, ax = plt.subplots(figsize=(10, 6))\n            y_pos = np.arange(len(class_names))\n            ax.barh(y_pos, probs * 100, color='skyblue')\n            ax.set_yticks(y_pos)\n            ax.set_yticklabels(class_names)\n            ax.set_xlabel('Probability (%)')\n            ax.set_title('Class Probabilities')\n            ax.invert_yaxis()\n            st.pyplot(fig)\n\nst.sidebar.header(\"â„¹ï¸ Instructions\")\nst.sidebar.write(\"\"\"\n1. Upload a clear image of a skin lesion\n2. Wait for the model to process the image\n3. View the prediction results and probabilities\n4. The app shows the top 3 most likely diagnoses\n\n**Note:** This is for educational purposes only. \nAlways consult a healthcare professional for medical diagnosis.\n\"\"\")\n\nst.sidebar.header(\"ðŸ¥ Disease Classes\")\nst.sidebar.write(\"\"\"\n- Chickenpox\n- Cowpox  \n- HFMD (Hand Foot Mouth Disease)\n- Healthy\n- Measles\n- Monkeypox\n\"\"\")\n'''\n    \n    with open('/kaggle/working/streamlit_app.py', 'w') as f:\n        f.write(app_code)\n\ndef main():\n    print(\"Creating datasets...\")\n    train_dataset, val_dataset, test_dataset, class_names = create_datasets()\n    \n    print(\"Training baseline ResNet18 model...\")\n    baseline_model = BaselineModel(num_classes=len(class_names)).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(baseline_model.parameters(), lr=1e-4, weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=20)\n    early_stopping = EarlyStopping(patience=7, verbose=True)\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n    \n    baseline_history = train_model(\n        baseline_model, train_loader, val_loader, criterion, optimizer, \n        scheduler, early_stopping, epochs=30, device=device\n    )\n    \n    print(\"Running Optuna hyperparameter optimization...\")\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=3, timeout=1200)\n    \n    if len(study.trials) > 0:\n        print(\"Best hyperparameters:\")\n        for key, value in study.best_params.items():\n            print(f\"  {key}: {value}\")\n        \n        best_params = study.best_params\n    else:\n        print(\"No successful trials. Using default parameters.\")\n        best_params = {'lr': 1e-4, 'weight_decay': 1e-4, 'dropout': 0.1, 'transformer_depth': 4}\n    \n    print(\"Training final hybrid model with best hyperparameters...\")\n    final_model = HybridViTModel(\n        num_classes=len(class_names),\n        transformer_depth=best_params['transformer_depth'],\n        dropout=best_params['dropout']\n    ).to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(\n        final_model.parameters(), \n        lr=best_params['lr'], \n        weight_decay=best_params['weight_decay']\n    )\n    scheduler = CosineAnnealingLR(optimizer, T_max=20)\n    early_stopping = EarlyStopping(patience=10, verbose=True)\n    \n    final_history = train_model(\n        final_model, train_loader, val_loader, criterion, optimizer,\n        scheduler, early_stopping, epochs=30, device=device\n    )\n    \n    print(\"Loading best model for evaluation...\")\n    final_model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n    \n    print(\"Evaluating on test set...\")\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n    metrics, predictions, targets, probabilities = evaluate_model(\n        final_model, test_loader, device, class_names\n    )\n    \n    print(\"Saving artifacts...\")\n    with open('/kaggle/working/artifacts/class_names.json', 'w') as f:\n        json.dump(class_names, f)\n    \n    with open('/kaggle/working/artifacts/metrics.json', 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    train_transform, val_transform = get_transforms()\n    with open('/kaggle/working/artifacts/transforms.pkl', 'wb') as f:\n        pickle.dump({'train': train_transform, 'val': val_transform}, f)\n    \n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(final_history['train_losses'], label='Train Loss')\n    plt.plot(final_history['val_losses'], label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(final_history['train_accs'], label='Train Acc')\n    plt.plot(final_history['val_accs'], label='Val Acc')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.title('Training and Validation Accuracy')\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/artifacts/training_history.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(\"Creating Streamlit app...\")\n    create_streamlit_app(class_names)\n    \n    print(\"All tasks completed successfully!\")\n    print(f\"Test Accuracy: {metrics['accuracy']:.2f}%\")\n    print(f\"Macro AUC: {metrics['macro_auc']:.4f}\")\n    print(\"Artifacts saved to /kaggle/working/artifacts/\")\n    print(\"Streamlit app saved to /kaggle/working/streamlit_app.py\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T13:36:38.185606Z","iopub.execute_input":"2025-11-14T13:36:38.186217Z","iopub.status.idle":"2025-11-14T14:31:55.914738Z","shell.execute_reply.started":"2025-11-14T13:36:38.186162Z","shell.execute_reply":"2025-11-14T14:31:55.913850Z"},"editable":false},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nCreating datasets...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"name":"stdout","text":"Training baseline ResNet18 model...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 184MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:36<00:00,  7.83it/s, Loss=0.4914, Acc=82.64%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:04<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4914, Train Acc: 82.64%\nVal Loss: 0.1976, Val Acc: 93.81%\nLearning Rate: 0.000099\n--------------------------------------------------\nValidation loss decreased (inf --> 0.197569). Saving model...\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.68it/s, Loss=0.1931, Acc=93.49%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1931, Train Acc: 93.49%\nVal Loss: 0.0840, Val Acc: 97.26%\nLearning Rate: 0.000098\n--------------------------------------------------\nValidation loss decreased (0.197569 --> 0.084030). Saving model...\nEpoch 3/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.64it/s, Loss=0.1180, Acc=96.11%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1180, Train Acc: 96.11%\nVal Loss: 0.0609, Val Acc: 98.05%\nLearning Rate: 0.000095\n--------------------------------------------------\nValidation loss decreased (0.084030 --> 0.060851). Saving model...\nEpoch 4/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.72it/s, Loss=0.0981, Acc=96.73%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0981, Train Acc: 96.73%\nVal Loss: 0.0580, Val Acc: 97.88%\nLearning Rate: 0.000090\n--------------------------------------------------\nValidation loss decreased (0.060851 --> 0.058021). Saving model...\nEpoch 5/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.62it/s, Loss=0.0880, Acc=96.90%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0880, Train Acc: 96.90%\nVal Loss: 0.0550, Val Acc: 98.41%\nLearning Rate: 0.000085\n--------------------------------------------------\nValidation loss decreased (0.058021 --> 0.054975). Saving model...\nEpoch 6/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.67it/s, Loss=0.0680, Acc=97.90%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 16.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0680, Train Acc: 97.90%\nVal Loss: 0.0451, Val Acc: 98.14%\nLearning Rate: 0.000079\n--------------------------------------------------\nValidation loss decreased (0.054975 --> 0.045115). Saving model...\nEpoch 7/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.45it/s, Loss=0.0427, Acc=98.60%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 16.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0427, Train Acc: 98.60%\nVal Loss: 0.0593, Val Acc: 98.14%\nLearning Rate: 0.000073\n--------------------------------------------------\nEarlyStopping counter: 1 out of 7\nEpoch 8/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.44it/s, Loss=0.0493, Acc=98.40%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0493, Train Acc: 98.40%\nVal Loss: 0.0315, Val Acc: 98.85%\nLearning Rate: 0.000065\n--------------------------------------------------\nValidation loss decreased (0.045115 --> 0.031479). Saving model...\nEpoch 9/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.77it/s, Loss=0.0374, Acc=98.97%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0374, Train Acc: 98.97%\nVal Loss: 0.0335, Val Acc: 99.03%\nLearning Rate: 0.000058\n--------------------------------------------------\nEarlyStopping counter: 1 out of 7\nEpoch 10/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.58it/s, Loss=0.0361, Acc=98.94%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0361, Train Acc: 98.94%\nVal Loss: 0.0360, Val Acc: 98.85%\nLearning Rate: 0.000050\n--------------------------------------------------\nEarlyStopping counter: 2 out of 7\nEpoch 11/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.70it/s, Loss=0.0292, Acc=99.04%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0292, Train Acc: 99.04%\nVal Loss: 0.0260, Val Acc: 99.12%\nLearning Rate: 0.000042\n--------------------------------------------------\nValidation loss decreased (0.031479 --> 0.026003). Saving model...\nEpoch 12/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.52it/s, Loss=0.0216, Acc=99.36%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0216, Train Acc: 99.36%\nVal Loss: 0.0264, Val Acc: 98.85%\nLearning Rate: 0.000035\n--------------------------------------------------\nEarlyStopping counter: 1 out of 7\nEpoch 13/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.81it/s, Loss=0.0161, Acc=99.55%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 16.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0161, Train Acc: 99.55%\nVal Loss: 0.0181, Val Acc: 99.20%\nLearning Rate: 0.000027\n--------------------------------------------------\nValidation loss decreased (0.026003 --> 0.018114). Saving model...\nEpoch 14/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.82it/s, Loss=0.0156, Acc=99.65%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0156, Train Acc: 99.65%\nVal Loss: 0.0111, Val Acc: 99.56%\nLearning Rate: 0.000021\n--------------------------------------------------\nValidation loss decreased (0.018114 --> 0.011144). Saving model...\nEpoch 15/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.27it/s, Loss=0.0181, Acc=99.54%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0181, Train Acc: 99.54%\nVal Loss: 0.0135, Val Acc: 99.47%\nLearning Rate: 0.000015\n--------------------------------------------------\nEarlyStopping counter: 1 out of 7\nEpoch 16/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.53it/s, Loss=0.0122, Acc=99.71%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 16.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0122, Train Acc: 99.71%\nVal Loss: 0.0154, Val Acc: 99.29%\nLearning Rate: 0.000010\n--------------------------------------------------\nEarlyStopping counter: 2 out of 7\nEpoch 17/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.68it/s, Loss=0.0106, Acc=99.74%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 16.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0106, Train Acc: 99.74%\nVal Loss: 0.0161, Val Acc: 99.38%\nLearning Rate: 0.000005\n--------------------------------------------------\nEarlyStopping counter: 3 out of 7\nEpoch 18/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.49it/s, Loss=0.0081, Acc=99.79%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0081, Train Acc: 99.79%\nVal Loss: 0.0087, Val Acc: 99.65%\nLearning Rate: 0.000002\n--------------------------------------------------\nValidation loss decreased (0.011144 --> 0.008725). Saving model...\nEpoch 19/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.42it/s, Loss=0.0059, Acc=99.91%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0059, Train Acc: 99.91%\nVal Loss: 0.0101, Val Acc: 99.65%\nLearning Rate: 0.000001\n--------------------------------------------------\nEarlyStopping counter: 1 out of 7\nEpoch 20/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.48it/s, Loss=0.0071, Acc=99.87%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0071, Train Acc: 99.87%\nVal Loss: 0.0085, Val Acc: 99.56%\nLearning Rate: 0.000000\n--------------------------------------------------\nValidation loss decreased (0.008725 --> 0.008518). Saving model...\nEpoch 21/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.37it/s, Loss=0.0088, Acc=99.82%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0088, Train Acc: 99.82%\nVal Loss: 0.0097, Val Acc: 99.47%\nLearning Rate: 0.000001\n--------------------------------------------------\nEarlyStopping counter: 1 out of 7\nEpoch 22/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.52it/s, Loss=0.0055, Acc=99.93%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 16.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0055, Train Acc: 99.93%\nVal Loss: 0.0107, Val Acc: 99.47%\nLearning Rate: 0.000002\n--------------------------------------------------\nEarlyStopping counter: 2 out of 7\nEpoch 23/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:20<00:00, 14.16it/s, Loss=0.0072, Acc=99.79%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0072, Train Acc: 99.79%\nVal Loss: 0.0115, Val Acc: 99.38%\nLearning Rate: 0.000005\n--------------------------------------------------\nEarlyStopping counter: 3 out of 7\nEpoch 24/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.82it/s, Loss=0.0081, Acc=99.80%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0081, Train Acc: 99.80%\nVal Loss: 0.0099, Val Acc: 99.65%\nLearning Rate: 0.000010\n--------------------------------------------------\nEarlyStopping counter: 4 out of 7\nEpoch 25/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.61it/s, Loss=0.0136, Acc=99.88%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0136, Train Acc: 99.88%\nVal Loss: 0.0130, Val Acc: 99.47%\nLearning Rate: 0.000015\n--------------------------------------------------\nEarlyStopping counter: 5 out of 7\nEpoch 26/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.50it/s, Loss=0.0115, Acc=99.86%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0115, Train Acc: 99.86%\nVal Loss: 0.0160, Val Acc: 99.47%\nLearning Rate: 0.000021\n--------------------------------------------------\nEarlyStopping counter: 6 out of 7\nEpoch 27/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:19<00:00, 14.62it/s, Loss=0.0134, Acc=99.56%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.91it/s]\n[I 2025-11-14 13:47:22,043] A new study created in memory with name: no-name-9b59502f-e4c7-4bcb-bc46-f28a16d9a72f\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0134, Train Acc: 99.56%\nVal Loss: 0.0242, Val Acc: 99.20%\nLearning Rate: 0.000027\n--------------------------------------------------\nEarlyStopping counter: 7 out of 7\nEarly stopping triggered\nRunning Optuna hyperparameter optimization...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 209MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:53<00:00,  5.33it/s, Loss=0.7166, Acc=74.68%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7166, Train Acc: 74.68%\nVal Loss: 0.2341, Val Acc: 92.22%\nLearning Rate: 0.000089\n--------------------------------------------------\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.44it/s, Loss=0.2878, Acc=90.73%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2878, Train Acc: 90.73%\nVal Loss: 0.2205, Val Acc: 93.90%\nLearning Rate: 0.000083\n--------------------------------------------------\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.41it/s, Loss=0.2099, Acc=93.33%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2099, Train Acc: 93.33%\nVal Loss: 0.1453, Val Acc: 95.84%\nLearning Rate: 0.000073\n--------------------------------------------------\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.44it/s, Loss=0.1580, Acc=95.13%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1580, Train Acc: 95.13%\nVal Loss: 0.0745, Val Acc: 98.05%\nLearning Rate: 0.000060\n--------------------------------------------------\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.42it/s, Loss=0.1132, Acc=96.62%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1132, Train Acc: 96.62%\nVal Loss: 0.0687, Val Acc: 98.23%\nLearning Rate: 0.000046\n--------------------------------------------------\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.42it/s, Loss=0.0758, Acc=97.47%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0758, Train Acc: 97.47%\nVal Loss: 0.0471, Val Acc: 98.59%\nLearning Rate: 0.000032\n--------------------------------------------------\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.42it/s, Loss=0.0449, Acc=98.53%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0449, Train Acc: 98.53%\nVal Loss: 0.0203, Val Acc: 99.47%\nLearning Rate: 0.000019\n--------------------------------------------------\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.42it/s, Loss=0.0366, Acc=99.03%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0366, Train Acc: 99.03%\nVal Loss: 0.0193, Val Acc: 99.20%\nLearning Rate: 0.000009\n--------------------------------------------------\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.45it/s, Loss=0.0324, Acc=99.38%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0324, Train Acc: 99.38%\nVal Loss: 0.0147, Val Acc: 99.47%\nLearning Rate: 0.000002\n--------------------------------------------------\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.44it/s, Loss=0.0208, Acc=99.32%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0208, Train Acc: 99.32%\nVal Loss: 0.0132, Val Acc: 99.47%\nLearning Rate: 0.000000\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-14 13:56:46,211] Trial 0 finished with value: -99.46949602122015 and parameters: {'lr': 9.141074723134877e-05, 'weight_decay': 1.7842534406785533e-06, 'dropout': 0.40446621963865914, 'transformer_depth': 5}. Best is trial 0 with value: -99.46949602122015.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.46it/s, Loss=0.7353, Acc=74.24%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7353, Train Acc: 74.24%\nVal Loss: 0.4012, Val Acc: 90.10%\nLearning Rate: 0.000088\n--------------------------------------------------\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.48it/s, Loss=0.2928, Acc=90.39%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2928, Train Acc: 90.39%\nVal Loss: 0.1708, Val Acc: 95.58%\nLearning Rate: 0.000082\n--------------------------------------------------\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.47it/s, Loss=0.1938, Acc=93.53%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1938, Train Acc: 93.53%\nVal Loss: 0.1898, Val Acc: 95.76%\nLearning Rate: 0.000072\n--------------------------------------------------\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.49it/s, Loss=0.1460, Acc=95.49%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1460, Train Acc: 95.49%\nVal Loss: 0.1419, Val Acc: 96.29%\nLearning Rate: 0.000059\n--------------------------------------------------\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.47it/s, Loss=0.0953, Acc=97.04%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0953, Train Acc: 97.04%\nVal Loss: 0.1026, Val Acc: 97.52%\nLearning Rate: 0.000045\n--------------------------------------------------\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.47it/s, Loss=0.0828, Acc=97.64%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0828, Train Acc: 97.64%\nVal Loss: 0.0659, Val Acc: 98.50%\nLearning Rate: 0.000031\n--------------------------------------------------\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.47it/s, Loss=0.0590, Acc=98.11%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0590, Train Acc: 98.11%\nVal Loss: 0.0368, Val Acc: 99.03%\nLearning Rate: 0.000019\n--------------------------------------------------\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.47it/s, Loss=0.0298, Acc=99.11%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0298, Train Acc: 99.11%\nVal Loss: 0.0314, Val Acc: 99.03%\nLearning Rate: 0.000009\n--------------------------------------------------\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.47it/s, Loss=0.0214, Acc=99.34%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0214, Train Acc: 99.34%\nVal Loss: 0.0217, Val Acc: 99.38%\nLearning Rate: 0.000002\n--------------------------------------------------\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.46it/s, Loss=0.0183, Acc=99.51%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.80it/s]\n[I 2025-11-14 14:06:02,749] Trial 1 finished with value: -99.38107869142353 and parameters: {'lr': 9.06442587803754e-05, 'weight_decay': 4.170321349351358e-06, 'dropout': 0.46867142925129457, 'transformer_depth': 3}. Best is trial 0 with value: -99.46949602122015.\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0183, Train Acc: 99.51%\nVal Loss: 0.0268, Val Acc: 99.38%\nLearning Rate: 0.000000\n--------------------------------------------------\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.44it/s, Loss=0.6175, Acc=78.33%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6175, Train Acc: 78.33%\nVal Loss: 0.2336, Val Acc: 92.40%\nLearning Rate: 0.000073\n--------------------------------------------------\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.46it/s, Loss=0.2392, Acc=91.98%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2392, Train Acc: 91.98%\nVal Loss: 0.0901, Val Acc: 96.91%\nLearning Rate: 0.000068\n--------------------------------------------------\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.45it/s, Loss=0.1494, Acc=95.01%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1494, Train Acc: 95.01%\nVal Loss: 0.0969, Val Acc: 97.08%\nLearning Rate: 0.000059\n--------------------------------------------------\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.46it/s, Loss=0.1066, Acc=96.48%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1066, Train Acc: 96.48%\nVal Loss: 0.0748, Val Acc: 97.35%\nLearning Rate: 0.000049\n--------------------------------------------------\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.46it/s, Loss=0.0785, Acc=97.52%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0785, Train Acc: 97.52%\nVal Loss: 0.0436, Val Acc: 98.85%\nLearning Rate: 0.000037\n--------------------------------------------------\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.46it/s, Loss=0.0497, Acc=98.31%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0497, Train Acc: 98.31%\nVal Loss: 0.0349, Val Acc: 98.85%\nLearning Rate: 0.000026\n--------------------------------------------------\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.47it/s, Loss=0.0404, Acc=98.98%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0404, Train Acc: 98.98%\nVal Loss: 0.0262, Val Acc: 99.29%\nLearning Rate: 0.000015\n--------------------------------------------------\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.46it/s, Loss=0.0300, Acc=99.05%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0300, Train Acc: 99.05%\nVal Loss: 0.0145, Val Acc: 99.56%\nLearning Rate: 0.000007\n--------------------------------------------------\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.46it/s, Loss=0.0216, Acc=99.34%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0216, Train Acc: 99.34%\nVal Loss: 0.0111, Val Acc: 99.73%\nLearning Rate: 0.000002\n--------------------------------------------------\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.46it/s, Loss=0.0216, Acc=99.51%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0216, Train Acc: 99.51%\nVal Loss: 0.0103, Val Acc: 99.82%\nLearning Rate: 0.000000\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-14 14:15:21,291] Trial 2 finished with value: -99.82316534040672 and parameters: {'lr': 7.485666798938327e-05, 'weight_decay': 3.900808104722941e-06, 'dropout': 0.15133338669152718, 'transformer_depth': 4}. Best is trial 2 with value: -99.82316534040672.\n","output_type":"stream"},{"name":"stdout","text":"Best hyperparameters:\n  lr: 7.485666798938327e-05\n  weight_decay: 3.900808104722941e-06\n  dropout: 0.15133338669152718\n  transformer_depth: 4\nTraining final hybrid model with best hyperparameters...\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.44it/s, Loss=0.5874, Acc=79.25%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5874, Train Acc: 79.25%\nVal Loss: 0.1630, Val Acc: 94.52%\nLearning Rate: 0.000074\n--------------------------------------------------\nValidation loss decreased (inf --> 0.162959). Saving model...\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.44it/s, Loss=0.2282, Acc=92.40%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2282, Train Acc: 92.40%\nVal Loss: 0.1682, Val Acc: 94.43%\nLearning Rate: 0.000073\n--------------------------------------------------\nEarlyStopping counter: 1 out of 10\nEpoch 3/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.44it/s, Loss=0.1856, Acc=93.71%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1856, Train Acc: 93.71%\nVal Loss: 0.0947, Val Acc: 96.55%\nLearning Rate: 0.000071\n--------------------------------------------------\nValidation loss decreased (0.162959 --> 0.094689). Saving model...\nEpoch 4/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.45it/s, Loss=0.1315, Acc=95.96%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1315, Train Acc: 95.96%\nVal Loss: 0.0800, Val Acc: 97.35%\nLearning Rate: 0.000068\n--------------------------------------------------\nValidation loss decreased (0.094689 --> 0.080047). Saving model...\nEpoch 5/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.45it/s, Loss=0.0891, Acc=96.88%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0891, Train Acc: 96.88%\nVal Loss: 0.0550, Val Acc: 97.88%\nLearning Rate: 0.000064\n--------------------------------------------------\nValidation loss decreased (0.080047 --> 0.054982). Saving model...\nEpoch 6/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.46it/s, Loss=0.0834, Acc=97.74%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0834, Train Acc: 97.74%\nVal Loss: 0.0564, Val Acc: 97.97%\nLearning Rate: 0.000059\n--------------------------------------------------\nEarlyStopping counter: 1 out of 10\nEpoch 7/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.46it/s, Loss=0.0783, Acc=97.48%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0783, Train Acc: 97.48%\nVal Loss: 0.0470, Val Acc: 98.67%\nLearning Rate: 0.000054\n--------------------------------------------------\nValidation loss decreased (0.054982 --> 0.047032). Saving model...\nEpoch 8/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.45it/s, Loss=0.0597, Acc=98.05%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0597, Train Acc: 98.05%\nVal Loss: 0.0233, Val Acc: 99.29%\nLearning Rate: 0.000049\n--------------------------------------------------\nValidation loss decreased (0.047032 --> 0.023305). Saving model...\nEpoch 9/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:52<00:00,  5.46it/s, Loss=0.0545, Acc=98.37%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0545, Train Acc: 98.37%\nVal Loss: 0.0509, Val Acc: 98.41%\nLearning Rate: 0.000043\n--------------------------------------------------\nEarlyStopping counter: 1 out of 10\nEpoch 10/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.47it/s, Loss=0.0508, Acc=98.36%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0508, Train Acc: 98.36%\nVal Loss: 0.0866, Val Acc: 97.70%\nLearning Rate: 0.000037\n--------------------------------------------------\nEarlyStopping counter: 2 out of 10\nEpoch 11/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.50it/s, Loss=0.6808, Acc=75.32%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6808, Train Acc: 75.32%\nVal Loss: 1.3744, Val Acc: 45.27%\nLearning Rate: 0.000032\n--------------------------------------------------\nEarlyStopping counter: 3 out of 10\nEpoch 12/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.55it/s, Loss=1.2910, Acc=51.63%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.2910, Train Acc: 51.63%\nVal Loss: 1.1207, Val Acc: 58.62%\nLearning Rate: 0.000026\n--------------------------------------------------\nEarlyStopping counter: 4 out of 10\nEpoch 13/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.52it/s, Loss=1.1176, Acc=58.90%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.1176, Train Acc: 58.90%\nVal Loss: 1.0472, Val Acc: 61.36%\nLearning Rate: 0.000020\n--------------------------------------------------\nEarlyStopping counter: 5 out of 10\nEpoch 14/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.55it/s, Loss=1.0291, Acc=62.32%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0291, Train Acc: 62.32%\nVal Loss: 0.9062, Val Acc: 66.93%\nLearning Rate: 0.000015\n--------------------------------------------------\nEarlyStopping counter: 6 out of 10\nEpoch 15/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.53it/s, Loss=0.9930, Acc=63.55%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9930, Train Acc: 63.55%\nVal Loss: 0.9455, Val Acc: 65.34%\nLearning Rate: 0.000011\n--------------------------------------------------\nEarlyStopping counter: 7 out of 10\nEpoch 16/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.51it/s, Loss=0.9872, Acc=62.65%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 14.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9872, Train Acc: 62.65%\nVal Loss: 0.8819, Val Acc: 67.20%\nLearning Rate: 0.000007\n--------------------------------------------------\nEarlyStopping counter: 8 out of 10\nEpoch 17/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.53it/s, Loss=0.9573, Acc=63.95%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9573, Train Acc: 63.95%\nVal Loss: 0.8713, Val Acc: 67.20%\nLearning Rate: 0.000004\n--------------------------------------------------\nEarlyStopping counter: 9 out of 10\nEpoch 18/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284/284 [00:51<00:00,  5.53it/s, Loss=0.9361, Acc=64.62%]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 13.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9361, Train Acc: 64.62%\nVal Loss: 0.8450, Val Acc: 67.90%\nLearning Rate: 0.000002\n--------------------------------------------------\nEarlyStopping counter: 10 out of 10\nEarly stopping triggered\nLoading best model for evaluation...\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:05<00:00,  6.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 99.29%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n  Chickenpox       0.97      0.98      0.98       113\n      Cowpox       0.99      0.99      0.99        99\n        HFMD       1.00      1.00      1.00       242\n     Healthy       1.00      0.99      1.00       171\n     Measles       1.00      0.99      0.99        83\n   Monkeypox       0.99      1.00      0.99       426\n\n    accuracy                           0.99      1134\n   macro avg       0.99      0.99      0.99      1134\nweighted avg       0.99      0.99      0.99      1134\n\nSaving artifacts...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Creating Streamlit app...\nAll tasks completed successfully!\nTest Accuracy: 99.29%\nMacro AUC: 0.9998\nArtifacts saved to /kaggle/working/artifacts/\nStreamlit app saved to /kaggle/working/streamlit_app.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n                           classification_report, confusion_matrix, cohen_kappa_score,\n                           matthews_corrcoef, log_loss, roc_auc_score, roc_curve, \n                           precision_recall_curve, auc)\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.manifold import TSNE\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms, datasets\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\nos.makedirs('/kaggle/working/plots', exist_ok=True)\nos.makedirs('/kaggle/working/misclassifications', exist_ok=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nclass HybridViTModel(nn.Module):\n    def __init__(self, num_classes=6, transformer_depth=4, dropout=0.1):\n        super(HybridViTModel, self).__init__()\n        self.resnet = torchvision.models.resnet50(weights=None)\n        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n        \n        self.feature_dim = 2048\n        self.patch_size = 7\n        self.num_patches = (224 // self.patch_size) ** 2\n        self.patch_embed_dim = 512\n        \n        self.patch_embed = nn.Conv2d(\n            self.feature_dim, self.patch_embed_dim, \n            kernel_size=self.patch_size, stride=self.patch_size\n        )\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.patch_embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.patch_embed_dim))\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.patch_embed_dim,\n            nhead=8,\n            dim_feedforward=1024,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_depth)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(self.patch_embed_dim, num_classes)\n        \n        self.gradients = None\n        self.activations = None\n        \n    def activations_hook(self, grad):\n        self.gradients = grad\n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        features = self.resnet(x)\n        \n        if self.training:\n            h = features.register_hook(self.activations_hook)\n        self.activations = features\n        \n        patch_embeds = self.patch_embed(features)\n        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n        \n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, patch_embeds), dim=1)\n        \n        if x.size(1) != self.pos_embed.size(1):\n            pos_embed = self.pos_embed[:, :x.size(1), :]\n        else:\n            pos_embed = self.pos_embed\n            \n        x = x + pos_embed\n        \n        x = self.dropout(x)\n        x = self.transformer(x)\n        \n        cls_output = x[:, 0]\n        output = self.classifier(cls_output)\n        \n        return output\n    \n    def get_activations_gradient(self):\n        return self.gradients\n    \n    def get_activations(self, x):\n        return self.resnet(x)\n\nclass AlbumentationsDataset(Dataset):\n    def __init__(self, file_paths, labels, transform=None):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        image = Image.open(self.file_paths[idx]).convert('RGB')\n        image = np.array(image)\n        label = self.labels[idx]\n        \n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n            \n        return image, label, self.file_paths[idx]\n\ndef get_transforms():\n    transform = A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    return transform\n\ndef load_model_and_data():\n    class_names = [\"Chickenpox\", \"Cowpox\", \"HFMD\", \"Healthy\", \"Measles\", \"Monkeypox\"]\n    \n    model = HybridViTModel(num_classes=len(class_names))\n    model.load_state_dict(torch.load('/kaggle/working/best_model.pth', map_location=device))\n    model.to(device)\n    model.eval()\n    \n    test_dir = '/kaggle/input/multiclass-skin-disease-dataset/Skin Lesion Dataset/test'\n    test_dataset = datasets.ImageFolder(test_dir)\n    \n    test_paths = [img[0] for img in test_dataset.imgs]\n    test_labels = [img[1] for img in test_dataset.imgs]\n    \n    transform = get_transforms()\n    test_albumentations_dataset = AlbumentationsDataset(test_paths, test_labels, transform)\n    \n    test_loader = DataLoader(test_albumentations_dataset, batch_size=32, shuffle=False, num_workers=2)\n    \n    return model, test_loader, class_names, test_paths\n\ndef evaluate_model(model, test_loader, class_names):\n    model.eval()\n    all_predictions = []\n    all_targets = []\n    all_probabilities = []\n    all_features = []\n    all_file_paths = []\n    \n    with torch.no_grad():\n        for data, target, file_paths in tqdm(test_loader, desc='Evaluating'):\n            data, target = data.to(device), target.to(device)\n            \n            output = model(data)\n            probabilities = torch.softmax(output, dim=1)\n            _, predicted = output.max(1)\n            \n            features = model.get_activations(data)\n            features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n            features = features.view(features.size(0), -1)\n            \n            all_predictions.extend(predicted.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n            all_probabilities.extend(probabilities.cpu().numpy())\n            all_features.extend(features.cpu().numpy())\n            all_file_paths.extend(file_paths)\n    \n    all_predictions = np.array(all_predictions)\n    all_targets = np.array(all_targets)\n    all_probabilities = np.array(all_probabilities)\n    all_features = np.array(all_features)\n    \n    accuracy = accuracy_score(all_targets, all_predictions)\n    precision_macro = precision_score(all_targets, all_predictions, average='macro', zero_division=0)\n    precision_micro = precision_score(all_targets, all_predictions, average='micro', zero_division=0)\n    precision_weighted = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n    \n    recall_macro = recall_score(all_targets, all_predictions, average='macro', zero_division=0)\n    recall_micro = recall_score(all_targets, all_predictions, average='micro', zero_division=0)\n    recall_weighted = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n    \n    f1_macro = f1_score(all_targets, all_predictions, average='macro', zero_division=0)\n    f1_micro = f1_score(all_targets, all_predictions, average='micro', zero_division=0)\n    f1_weighted = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n    \n    kappa = cohen_kappa_score(all_targets, all_predictions)\n    mcc = matthews_corrcoef(all_targets, all_predictions)\n    loss = log_loss(all_targets, all_probabilities)\n    \n    y_test_bin = label_binarize(all_targets, classes=range(len(class_names)))\n    \n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    for i in range(len(class_names)):\n        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], all_probabilities[:, i])\n        roc_auc[i] = roc_auc_score(y_test_bin[:, i], all_probabilities[:, i])\n    \n    macro_auc = roc_auc_score(y_test_bin, all_probabilities, average='macro')\n    micro_auc = roc_auc_score(y_test_bin, all_probabilities, average='micro')\n    \n    precision_dict = dict()\n    recall_dict = dict()\n    pr_auc = dict()\n    \n    for i in range(len(class_names)):\n        precision_dict[i], recall_dict[i], _ = precision_recall_curve(y_test_bin[:, i], all_probabilities[:, i])\n        pr_auc[i] = auc(recall_dict[i], precision_dict[i])\n    \n    per_class_accuracy = []\n    for i in range(len(class_names)):\n        class_mask = all_targets == i\n        if np.sum(class_mask) > 0:\n            class_acc = accuracy_score(all_targets[class_mask], all_predictions[class_mask])\n            per_class_accuracy.append(class_acc)\n        else:\n            per_class_accuracy.append(0.0)\n    \n    metrics = {\n        'accuracy': float(accuracy),\n        'precision': {\n            'macro': float(precision_macro),\n            'micro': float(precision_micro),\n            'weighted': float(precision_weighted)\n        },\n        'recall': {\n            'macro': float(recall_macro),\n            'micro': float(recall_micro),\n            'weighted': float(recall_weighted)\n        },\n        'f1_score': {\n            'macro': float(f1_macro),\n            'micro': float(f1_micro),\n            'weighted': float(f1_weighted)\n        },\n        'cohens_kappa': float(kappa),\n        'matthews_correlation': float(mcc),\n        'log_loss': float(loss),\n        'auc': {\n            'macro': float(macro_auc),\n            'micro': float(micro_auc),\n            'per_class': {class_names[i]: float(roc_auc[i]) for i in range(len(class_names))}\n        },\n        'per_class_accuracy': {class_names[i]: float(per_class_accuracy[i]) for i in range(len(class_names))}\n    }\n    \n    return (metrics, all_predictions, all_targets, all_probabilities, \n            all_features, all_file_paths, fpr, tpr, roc_auc, precision_dict, recall_dict, pr_auc)\n\ndef generate_gradcam(model, image_tensor, target_class, class_names):\n    model.eval()\n    \n    def backward_hook(module, grad_in, grad_out):\n        model.gradients = grad_out[0]\n    \n    def forward_hook(module, input, output):\n        model.activations = output\n    \n    handle_forward = model.resnet[-1].register_forward_hook(forward_hook)\n    handle_backward = model.resnet[-1].register_backward_hook(backward_hook)\n    \n    image_tensor = image_tensor.unsqueeze(0).to(device)\n    image_tensor.requires_grad_()\n    \n    output = model(image_tensor)\n    model.zero_grad()\n    \n    one_hot_output = torch.zeros(output.size()).to(device)\n    one_hot_output[0, target_class] = 1\n    output.backward(gradient=one_hot_output)\n    \n    gradients = model.gradients\n    activations = model.activations\n    \n    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n    \n    for i in range(activations.size(1)):\n        activations[:, i, :, :] *= pooled_gradients[i]\n    \n    heatmap = torch.mean(activations, dim=1).squeeze()\n    heatmap = torch.nn.functional.relu(heatmap)\n    heatmap /= torch.max(heatmap)\n    \n    handle_forward.remove()\n    handle_backward.remove()\n    \n    return heatmap.cpu().detach().numpy()\n\ndef plot_gradcam(model, test_loader, class_names, target_classes=['Monkeypox', 'Measles', 'Healthy']):\n    model.eval()\n    target_indices = [class_names.index(cls) for cls in target_classes]\n    \n    found_samples = {cls: None for cls in target_classes}\n    \n    for data, targets, file_paths in test_loader:\n        data = data.to(device)\n        targets = targets.cpu().numpy()\n        \n        for i, target in enumerate(targets):\n            if class_names[target] in target_classes and found_samples[class_names[target]] is None:\n                found_samples[class_names[target]] = (data[i], target, file_paths[i])\n        \n        if all(found_samples[cls] is not None for cls in target_classes):\n            break\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    \n    for idx, (cls_name, sample_data) in enumerate(found_samples.items()):\n        if sample_data is None:\n            continue\n            \n        image_tensor, true_class, file_path = sample_data\n        class_idx = class_names.index(cls_name)\n        \n        heatmap = generate_gradcam(model, image_tensor, class_idx, class_names)\n        \n        original_image = Image.open(file_path).convert('RGB')\n        original_image = original_image.resize((224, 224))\n        original_img = np.array(original_image)\n        \n        heatmap_resized = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n        heatmap_resized = np.uint8(255 * heatmap_resized)\n        heatmap_colored = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n        \n        superimposed_img = heatmap_colored * 0.4 + original_img * 0.6\n        superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n        \n        axes[0, idx].imshow(original_img)\n        axes[0, idx].set_title(f'Original: {cls_name}')\n        axes[0, idx].axis('off')\n        \n        axes[1, idx].imshow(superimposed_img)\n        axes[1, idx].set_title(f'Grad-CAM: {cls_name}')\n        axes[1, idx].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/plots/gradcam_comparison.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef plot_tsne(features, targets, class_names):\n    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n    features_2d = tsne.fit_transform(features)\n    \n    plt.figure(figsize=(12, 10))\n    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=targets, cmap='tab10', alpha=0.7)\n    plt.colorbar(scatter)\n    plt.title('t-SNE Visualization of Feature Embeddings')\n    \n    for i, class_name in enumerate(class_names):\n        class_indices = np.where(targets == i)[0]\n        if len(class_indices) > 0:\n            centroid = np.mean(features_2d[class_indices], axis=0)\n            plt.annotate(class_name, centroid, xytext=(5, 5), textcoords='offset points', \n                        fontsize=9, alpha=0.8)\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/plots/tsne_features.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef analyze_misclassifications(all_predictions, all_targets, all_file_paths, class_names):\n    misclassified_indices = np.where(all_predictions != all_targets)[0]\n    \n    misclassification_data = []\n    for idx in misclassified_indices:\n        misclassification_data.append({\n            'file_path': all_file_paths[idx],\n            'true_label': class_names[all_targets[idx]],\n            'predicted_label': class_names[all_predictions[idx]],\n            'true_index': int(all_targets[idx]),\n            'predicted_index': int(all_predictions[idx])\n        })\n    \n    misclassification_df = pd.DataFrame(misclassification_data)\n    \n    plt.figure(figsize=(15, 12))\n    num_samples = min(20, len(misclassified_indices))\n    \n    for i, idx in enumerate(misclassified_indices[:num_samples]):\n        plt.subplot(4, 5, i + 1)\n        \n        image = Image.open(all_file_paths[idx])\n        plt.imshow(image)\n        plt.title(f'True: {class_names[all_targets[idx]]}\\nPred: {class_names[all_predictions[idx]]}', \n                 fontsize=8)\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/plots/misclassified_samples.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    return misclassification_df\n\ndef create_visualizations(metrics, all_predictions, all_targets, all_probabilities, \n                         class_names, fpr, tpr, roc_auc, precision_dict, recall_dict, pr_auc):\n    \n    cm = confusion_matrix(all_targets, all_predictions)\n    cm_normalized = confusion_matrix(all_targets, all_predictions, normalize='true')\n    \n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    \n    plt.subplot(1, 2, 2)\n    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title('Normalized Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/plots/confusion_matrices.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    plt.figure(figsize=(10, 8))\n    colors = plt.cm.Set3(np.linspace(0, 1, len(class_names)))\n    \n    for i, color in zip(range(len(class_names)), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Multi-class ROC Curves')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('/kaggle/working/plots/roc_curve.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    plt.figure(figsize=(10, 8))\n    for i, color in zip(range(len(class_names)), colors):\n        plt.plot(recall_dict[i], precision_dict[i], color=color, lw=2,\n                label=f'{class_names[i]} (AUC = {pr_auc[i]:.2f})')\n    \n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curves')\n    plt.legend(loc=\"lower left\")\n    plt.savefig('/kaggle/working/plots/pr_curve.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    plt.figure(figsize=(10, 6))\n    metrics_to_plot = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n    values = [metrics['accuracy'], metrics['precision']['macro'], \n              metrics['recall']['macro'], metrics['f1_score']['macro']]\n    \n    plt.bar(metrics_to_plot, values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n    plt.title('Key Performance Metrics')\n    plt.ylabel('Score')\n    plt.ylim(0, 1)\n    \n    for i, v in enumerate(values):\n        plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n    \n    plt.savefig('/kaggle/working/plots/key_metrics.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef main():\n    print(\"Loading model and data...\")\n    model, test_loader, class_names, test_paths = load_model_and_data()\n    \n    print(\"Evaluating model...\")\n    (metrics, all_predictions, all_targets, all_probabilities, \n     all_features, all_file_paths, fpr, tpr, roc_auc, \n     precision_dict, recall_dict, pr_auc) = evaluate_model(model, test_loader, class_names)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"EVALUATION RESULTS\")\n    print(\"=\"*50)\n    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n    print(f\"Macro F1: {metrics['f1_score']['macro']:.4f}\")\n    print(f\"Cohen's Kappa: {metrics['cohens_kappa']:.4f}\")\n    print(f\"Macro AUC: {metrics['auc']['macro']:.4f}\")\n    print(f\"Log Loss: {metrics['log_loss']:.4f}\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(all_targets, all_predictions, target_names=class_names))\n    \n    print(\"\\nPer-class Accuracy:\")\n    for class_name, acc in metrics['per_class_accuracy'].items():\n        print(f\"{class_name}: {acc:.4f}\")\n    \n    print(\"\\nPer-class AUC:\")\n    for class_name, auc_val in metrics['auc']['per_class'].items():\n        print(f\"{class_name}: {auc_val:.4f}\")\n    \n    print(\"\\nGenerating visualizations...\")\n    create_visualizations(metrics, all_predictions, all_targets, all_probabilities, \n                         class_names, fpr, tpr, roc_auc, precision_dict, recall_dict, pr_auc)\n    \n    print(\"Generating Grad-CAM visualizations...\")\n    plot_gradcam(model, test_loader, class_names)\n    \n    print(\"Generating t-SNE visualization...\")\n    plot_tsne(all_features, all_targets, class_names)\n    \n    print(\"Analyzing misclassifications...\")\n    misclassification_df = analyze_misclassifications(all_predictions, all_targets, \n                                                     all_file_paths, class_names)\n    \n    print(\"Saving results...\")\n    with open('/kaggle/working/metrics.json', 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    misclassification_df.to_csv('/kaggle/working/misclassifications/misclassified_samples.csv', index=False)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"EVALUATION COMPLETE\")\n    print(\"=\"*50)\n    print(f\"Results saved to:\")\n    print(f\"- Metrics: /kaggle/working/metrics.json\")\n    print(f\"- Plots: /kaggle/working/plots/\")\n    print(f\"- Misclassifications: /kaggle/working/misclassifications/\")\n    print(f\"Total misclassified samples: {len(misclassification_df)}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:28:22.385892Z","iopub.execute_input":"2025-11-14T15:28:22.386593Z","iopub.status.idle":"2025-11-14T15:28:47.901728Z","shell.execute_reply.started":"2025-11-14T15:28:22.386560Z","shell.execute_reply":"2025-11-14T15:28:47.900847Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading model and data...\nEvaluating model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:09<00:00,  3.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEVALUATION RESULTS\n==================================================\nAccuracy: 0.9929\nMacro F1: 0.9916\nCohen's Kappa: 0.9908\nMacro AUC: 0.9998\nLog Loss: 0.0226\n\nClassification Report:\n              precision    recall  f1-score   support\n\n  Chickenpox       0.97      0.98      0.98       113\n      Cowpox       0.99      0.99      0.99        99\n        HFMD       1.00      1.00      1.00       242\n     Healthy       1.00      0.99      1.00       171\n     Measles       1.00      0.99      0.99        83\n   Monkeypox       0.99      1.00      0.99       426\n\n    accuracy                           0.99      1134\n   macro avg       0.99      0.99      0.99      1134\nweighted avg       0.99      0.99      0.99      1134\n\n\nPer-class Accuracy:\nChickenpox: 0.9823\nCowpox: 0.9899\nHFMD: 0.9959\nHealthy: 0.9942\nMeasles: 0.9880\nMonkeypox: 0.9953\n\nPer-class AUC:\nChickenpox: 0.9992\nCowpox: 0.9999\nHFMD: 1.0000\nHealthy: 1.0000\nMeasles: 1.0000\nMonkeypox: 0.9999\n\nGenerating visualizations...\nGenerating Grad-CAM visualizations...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n","output_type":"stream"},{"name":"stdout","text":"Generating t-SNE visualization...\nAnalyzing misclassifications...\nSaving results...\n\n==================================================\nEVALUATION COMPLETE\n==================================================\nResults saved to:\n- Metrics: /kaggle/working/metrics.json\n- Plots: /kaggle/working/plots/\n- Misclassifications: /kaggle/working/misclassifications/\nTotal misclassified samples: 8\n","output_type":"stream"}],"execution_count":3}]}